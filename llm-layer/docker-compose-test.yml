services:
  llm-layer-inference:
    container_name: llm-layer-inference
    image: vllm/vllm-openai:latest
    ports:
      - "8000:8000"
    volumes:
      - llm-layer-inference-data:/models
    environment:
      - MODEL_NAME=meta-llama/Llama-2-7b-chat-hf
      - TENSOR_PARALLEL_SIZE=1
      - MAX_MODEL_LEN=4096
      - QUANTIZATION=awq
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - app-network

volumes:
  llm-layer-inference-data:

networks:
  app-network:
    external: true
    name: communication-layer_app-network 